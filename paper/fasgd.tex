\documentclass{article} % For LaTeX2e
\usepackage{iclr2016_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{color}

\title{\scalebox{1}{Faster Asynchronous SGD}}

\author{
\hspace{20mm} Augustus Odena \\
\hspace{10mm} \texttt{augustus.odena@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand\RR{\mathbb{R}}
\newcommand\CC{\mathbb{C}}
\newtheorem{lemma}{Lemma}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\renewcommand{\baselinestretch}{0.98125}

\begin{document}

\begin{center}
\maketitle
\end{center}

\begin{abstract}

Asynchronous distributed stochastic gradient descent methods have trouble converging because of stale gradients.
A gradient update sent to a parameter server by a client is stale if the parameters used to calculate that gradient have since been updated on the server.
Approaches have been proposed to circumvent this problem that quantify staleness in terms of the number of elapsed updates.
In this work, we propose a method that quantifies staleness in terms of trailing gradient statistics.
We show that this method outperforms previous methods with respect to convergence speed and scalability to many clients.
We also discuss how an extension to this method can be used to dramatically reduce bandwidth costs
in a distributed training context.
In particular, our method allows reduction of total bandwidth usage by a factor of 25 with little impact on
cost convergence.
We also describe (and link to) a software library that we have used to simulate these algorithms
deterministically on a single machine.

\end{abstract}

\section{Introduction}
Neural Networks can be trained on multiple machines simultaneously using distributed stochastic gradient descent (SGD).
To accomplish this, a replica of the model is created on each separate machine. These replica machines are called clients.
One machine holds the canonical snapshot of the model parameters, and this machine is called the server.
Clients take a random mini-batch of training data and perform backpropagation on their replica of the model to maintain a gradient estimate.
This estimate is then sent back to the server to be applied to the global parameters.
If the server waits to collect all of the client updates before applying them to its parameters, we say that the SGD is synchronous.
If the server applies updates as they come in (as first introduced in \cite{FirstASGD}), we say that the SGD is asynchronous.
Each method has pros and cons.
Synchronous SGD is slower because clients spend some time waiting, but it's free from convergence issues.
Async SGD removes the waiting, but may have trouble converging due to stale gradients.

In \cite{Suyog} they propose a type of staleness aware async SGD.
In the context of asynchronous gradient updates in a parameter server context, they say a gradient
sent to the parameter server from a worker is stale by k steps if the weights used to compute that
gradient have since been updated k times.

They then describe a policy of modulating the learning rate as a function of staleness.
This is accomplished by simply dividing the gradient update by the staleness before applying it.
We refer to that strategy as staleness-aware async SGD (SASGD).

SASGD outperforms ASGD in terms of convergence, but it leaves performance on the table by treating all gradients as equal when computing staleness.
In this paper, we exploit this slack by modulating the learning rate as a function of trailing gradient statistics.
Exploiting this slack yields to better convergence properties and reduced bandwidth use.
We call this new algorithm Faster ASGD (FASGD for short).

The rest of the paper is organized as follows: first we discuss the FASGD algorithm.
We give some theoretical characterizations of the convergence properties and bandwidth use.
Then we introduce our experimental architecture before finally finishing with the experimental results.

TODO: mention more related work.

\section{The FASGD Algorithm}

We first introduce existing staleness aware algorithms before moving on to FASGD and variants.

\subsection{Background}
We adopt the notation from \cite{Suyog} for describing an asynchronous training session,
with some slight modifications:

\begin{itemize}
  \item $\lambda$: The number of clients.
  \item $\mu$: The minibatch size used by all learners to produce stochastic gradient estimates.
  \item $\alpha$: The master learning rate. Individual learning rates are made by modifications to this.
  \item $T$: The scalar timestamp that tracks how many times pending gradients have been applied to the
    master parameters. The timestamp starts at 0 and is incremented by one for each weight update (regardless)
    of the number of clients or the sizes of the minibatches involved in computing the update. By the timestamp
    of a gradient, we mean the timestamp of the parameters used to compute that gradient.
  \item $\tau_{i,l}$: The step-staleness of the stochastic gradient from client $l$ with respect to the parameters timestamp
    $i$. A client $l$ pushes gradient with timestamp $j$ to the parameter server with timestamp $i$, where $i \geq j$.
    We calculate the step-staleness $\tau_{i,l}$ of this gradient as $iâˆ’j$.
    The step-staleness is always non-negative as defined.
\end{itemize}

Since there are many possible implementations of Asynchronous SGD, we formally specify the algorithm used for
the purposes of this paper:
TODO: get rid of softsync description while keeping alpha and stuff.

\begin{itemize}
\item Async SGD Protocol: In this protocol, all $\lambda$ clients compute gradients asynchronously in parallel.
  When a client $l$ is done computing gradients $\Delta \theta_l$,
  it waits to take the lock on the parameter server (only one client can communicate with the
  server at a time). When the client takes the lock, the following things happen atomically before the lock is released:
  \begin{enumerate}
    \item Client $l$ passes the computed stochastic gradient $\Delta \theta_l$ to the server.
    \item The server updates the global parameters $\theta_i$ according to the following equation:
      $ \theta_{i + 1} = \theta_{i} - \alpha \Delta \theta_l$
    \item The server passes the updated parameters $\theta_{i + 1}$ back to client $l$. 
  \end{enumerate}

\item N-SoftSync Protocol: In this protocol, each learner $l$ pulls the weights from the parameter server,
  calculates the gradients and pushes the gradients to the parameter server.
  The parameter server updates the weights after collecting exactly
  $c = \left \lfloor{\frac{\lambda}{n}}\right \rfloor $
  gradients from any of the $\lambda$ learners.
  It's still true that only one client communicates with the server at a time.
  If a client submits a gradient that doesn't result in new parameters,
  it will block and wait for the new parameters to arrive.
  I don't think that this is exactly how the other paper does it,
  but if we just compare for $\lambda = n$, then it doesn't matter,
  because all gradients result in a parameter update in that case.
  The splitting parameter n can vary from 1 to $\lambda$.
  The n-softsync weight update rule is given by:

  $$c = \left \lfloor{\frac{\lambda}{n}}\right \rfloor $$

  $$  g_i = \frac{1}{c} \sum_{k = 1}^{c} \alpha(\tau_{i,k})\Delta \theta_k$$

  $$ \theta_{i + 1} = \theta_{i} - g_i $$

  where $\alpha(\tau_{i,k})$ is the step-staleness dependent learning rate.  

  
\end{itemize}

In \cite{Suyog}, the suggestion is to define $\alpha(\tau_{i,k}) := \frac{\alpha}{\tau_{i,k}}$. 
This modification does allow convergence for high $\lambda$ values (these are the ones with lots of staleness),
but it's unclear that the convergence given is optimal - indeed, we show that it's not.

\subsection{Improvements}

One of the claims of this paper is that the step-staleness as defined in the last section
can be substantially improved upon as a measure of staleness.
In what follows, we introduce what we claim is a better measure. 

Consider a server with gradients $\theta_{i}$ and a client $l$ training on parameters $\theta_{j}$,
where $j \leq i$. Suppose that the client in question then sends a gradient estimate $\Delta \theta_l$
to the server with no intervening parameter updates.

We define the \textbf{True Staleness} $\Gamma(\theta_{i}, \Delta \theta_l)$ as follows:

$$ \Gamma(\theta_{i}, \Delta \theta_l) = || (\Delta \theta_l) - (\Delta \theta_i) || $$

Where $ \Delta \theta_l $ is the stochastic gradient computed by client $l$ and
$ \Delta \theta_i$ is the stochastic gradient that would have been computed with client
$l$ on \textit{the same minibatch} if client $l$ had used $\theta_i$ instead of $\theta_j$.

It might not be immediately apparent why this is a better measure, but we'll justify this below.

The basic idea is as follows: consider a client $l$ training on parameters $\theta_{i-2}$
while the server has parameters $\theta_{i}$.
If client $l$ is the next client to send its stochastic gradient to the server, then that
gradient $\Delta \theta_l$ will have step staleness $\tau_{i,l} = 2$.
Now in this case the copy of parameters on the master $\theta_i$ can be expressed as $\theta_i = \theta_{i-2} - g_1 - g_2$,
where $g_1$ and $g_2$ are the updates computed by multiplying some learning rate by the mean (across the minibatch) of
the the stochastic gradients returned by the client.

We care about staleness because the stochastic gradients computed using old parameters will be a biased estimate
of the true gradient at the current point in parameter space.

In particular, for any stale gradient, we can decompose its difference from the true gradient
into the difference due to using SGD rather than batch GD and the difference $(\Delta \theta_l) - (\Delta \theta_i)$
(kind of a bias-variance decomposition for Async SGD.).
Since we can't control the first component (except by changing minibatch size),
it's safe to say that we just care about the True Staleness.

However, not all pairs $g_1, g_2$ will cause the same amount of True Staleness.
If $g_1$ and $g_2$ largely cancel each other out, then $\Gamma$ will be less than if $g_1$ and $g_2$ mostly have the same sign.
If the hessian has large values at $\theta_i$, then $\Gamma$ will be higher than if the hessian has small values.
If we can account for these things in our learning algorithm, we should be able to do better aysnc-SGD.

True Staleness can be approximated (using the Taylor series expansion)
by how far we have moved in the parameter space times the rate of change
in the grads with respect to a change in parameter space.
Even this approximation is intractable to compute exactly because it involves higher order derivatives,
but to the extent we can approximate it,
we will be using a strictly more informative measure than before, and so should be able to improve convergence.

Taking inspiration from the version of RMSProp in \cite{Graves}, we can do the following:
Maintain a global trailing estimate of the standard deviation of each parameter tensor.
Define pointwise staleness of a particular tensor to refer to the number of missed updates weighted by the value of this
trailing estimate at the time those updates were applied.
At update time, modulate learning rate based on this new pointwise staleness measure.

More formally, we propose the following protocol, which we call FASGD:

\begin{itemize}
\item FASGD: In this protocol, each learner $l$ pulls the weights from the parameter server,
  calculates the gradients and pushes the gradients to the parameter server,
  just as in the n-softsync protocol defined earlier.
  As in n-softsync, the parameter server updates
  the weights after collecting exactly
  $c = \left \lfloor{\frac{\lambda}{n}}\right \rfloor $
  gradients from any of the $\lambda$ learners.
  The splitting parameter n can vary from 1 to $\lambda$.
  We maintain a trailing estimate of gradient variance as follows:

  $$ n_i = \gamma n_{i-1} + (1 - \gamma) \Delta \theta_l^2 $$

  $$ b_i = \gamma b_{i-1} + (1 - \gamma) \Delta \theta_l $$

  $$ v_i = \beta  v_{i-1} + (1 - \beta)  \frac{1}{\sqrt{n_i - b_i^2 + \epsilon}}$$
  
  where $\gamma$ and $\beta$ are hyperparameters and $\epsilon$ is for numerical
  stability.

  The FASGD weight update rule is then given by:

  $$c = \left \lfloor{\frac{\lambda}{n}}\right \rfloor $$

  $$  g_i = \frac{1}{c} \sum_{k = 1}^{c} \frac{\alpha}{v_i \tau_{i,k}}\Delta \theta_k$$

  $$ \theta_{i + 1} = \theta_{i} - g_i $$

  where $\alpha(\tau_{i,k})$ is the step-staleness dependent learning rate.  
  
\end{itemize}
From an intutive perspective, it would be nice to understand why
dividing the learning rate by the standard deviation helps us.
One way to think of this is as follows:
If the hessian has large values, we expect big changes from gradient to gradient,
so we should see high trailing gradient variance.
We also expect higher True Staleness in this instance,
so dividing by trailing gradient variance will help convergence.

We can also consider the potential for gradients to cancel each other out.
If grads cancel each other out, they must have changed sign.
Where there are sign changes, there is likely to be higher trailing gradient
variance (is this really true?).

Overall, FASGD should lead to better convergence for the following reasons:
It will allow us to keep the learning rate high when True staleness is less than step-staleness estimates.
It will prevent us from overshooting when True staleness is higher than step-stalensss estimates.

\subsection{Bandwidth Aware FASGD}

TODO: fill in a description of what we do here,
based on citing the original downpour paper and going from there.

move on to point out the extension
Finally, we'd like to point out that a modification of FASGD can be
made to reduce bandwidth usage in systems where this is desirable.
Once True Staleness is taken into account,
we may find that certain tensors get stale slower than other tensors.
Suppose that our model has two parameter tensors $T_1$ and $T_2$.
If we find that $T_1$ is getting stale twice as fast as $T_2$, we can do one of two things:
If we are bandwidth constrained, we can choose to transmit updates of $T_2$ less frequently.
If we are not bandwidth constrained, we can increase the learning rate for $T_2$ and transmit at the same rate.

\section{Experimental Setup}
\label{sec:impl}
\vspace{-0.1mm}
In this section, we describe FRED - a Python library for simulating distributed training algorithms
deterministically on a single node.
The source code for FRED is at \href{https://github.com/DoctorTeeth/fasgd}{https://github.com/DoctorTeeth/fasgd}.
Later, we'll describe how we used FRED to generate experimental results for FASGD.

FRED works by taking an idiomatic description of a distributed training algorithm and running it
deterministically on a single node. This is useful for the following reasons:

First, determinism is helpful for debugging and experimentation.
It allows us to check that runs which should be bitwise equivalent are bitwise equivalent.
For instance, we can check that hard-sync with $c$ clients and batch size $\mu$ is the same
algorithm as vanilla SGD with batch size $\frac{\mu}{c}$.
We would have dramatically less confidence in the correctness of our implementations without this feature.

Second, setting up a big training cluster is an ordeal.
It's hard and it's expensive, and differences in configurations are
likely to render academic results irreproducible.
Moreover, the quirks involved in a particular system may obscure algorithmic considerations.
Using a simulator allows you to test training algorithms for all topologies, since you
have full control over the order (which can be made probabilistic) in which updates are
sent from various clients.

To generate a FRED run, a user implements a Server class with a specific interface.
The implemented class governs how and when gradients will be applied to the Server Parameters.
More specifically, the Server class must implement and initialization function and an apply-update function.

The initialization function creates various data structures for keeping track of the pending updates.
For instance, in an n-softsync server, we might keep a set of (staleness, gradient) tuples that represents
all of the client updates seen since the last parameter update.

The apply-update function takes 3 arguments: a gradient update, the client ID for the client that generated
the update, and the time stamp of the parameters used by the client to generate the update.
Then the update is applied to the parameters in the appropriate way.
For instance, in a hardsync server, the apply update function is implemented with the
following Python code:

\begin{verbatim}

def apply_update(self, grads, timestamp, client):

        unblock = False
        self.pending_grads[client] = grads

        if len(self.pending_grads) == self.clients:

            # all grads is a list of lists of grads
            # each of which has the same length
            all_grads = self.pending_grads.values()

            # apply the param update
            for this_grad in all_grads:
                for g, p in zip(this_grad, self.params):
                    old_p = p.get_value()
                    mod = g / self.clients 
                    p.set_value(old_p - self.learning_rate * mod)

            self.timestamp += 1 # weights have changed
            unblock = True
            self.pending_grads = {}

        return self.params, self.timestamp, unblock
\end{verbatim}

Specifying the behavior of those two functions uniquely defines the behavior of a
FRED experiment.

The Server is then hooked up to a Dispatcher and a group of Clients.
The Dispatcher will manage the simulation of which gradient update come from what client when.
This simulation takes as an argument a rule determining each client's probability of being selected
and how that probability will change (I think this can just be implemented as a Markov P matrix). 

We've made at least one opinionated decision about the library that merits further discussion.
After a client pushes gradients to a server, it will wait for the resulting parameter update (or non-update) before
resuming its computation. This is good in some ways and bad in others. If the overhead of applying the updates is
small relative to that of computing the gradients, and/or if we care a lot about staleness, we may as well wait.
If we cared a lot about throughput instead, we might just work with whatever the most recently computed set of parameters is.

We may decide to make this configurable, or to come up with some more general framework in which the configurability
of this particular parameter comes for free.

\vspace{-0.1mm}

\section{Experimental Results}
\label{expts}

In what follows, we discuss experimental results generated using FRED.
We compare FASGD to SASGD and an ASGD baseline on a variety of standard tasks.

\subsection{FASGD}

Our first test is a 2 layer MLP trained on the MNIST task.
The MLP has 200 hidden units.
These use a relu activation, and the cost is negative log likelihood,
as is standard.

The training uses 120 clients with a batch size of 1.
The training runs for 100,000 iterations,
where each iteration corresponds to a client calclulating
a gradient estimate for a single minibatch.
We used learning rates of 0.01, 0.005, and 0.001.
FASGD performs meaningfully better (converging faster and to a better cost)
regardless of the learning rate.
This difference is also robust with respect to different random
initializtions.

One might object that FASGD only does better on this problem because
the optimal learning rate for this problem is higher, and because
FASGD tends to increase the effective learning rate by dividing
it by the magnitude of the grads, which tend to be less than 1.
However, if this were the only reason for the outperformance,
we'd expect SASGD to do better than FASGD at some higher learning
rate $L$ where $L$ is around the region in learning rate space
at which increases to learning rate stop yielding improvements
in convergence. That this does not happen is evidence of
FASGD's superiority as a general algorithm.

\begin{figure}[ht!]
  \centering
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/mnist_mlp_0_001.png}
  \end{minipage}%%
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/mnist_mlp_0_005.png}
  \end{minipage} 
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/mnist_mlp_0_01.png}
  \end{minipage}%% 
  \caption{MNIST Validation cost for FASGD (Blue) and SASGD (Green).
  FASGD outperforms SASGD for all tested learning rates. 
  Clockwise from the top left, the rates are 0.001, 0.005, 0.01.
  }
  \label{fig1} 
\end{figure}

We also tested how the difference between FASGD and SASGD changed as a function of the number of clients.
We tried client counts of 250, 500, 1000, and 10000.
This was with a minibatch size of 1 and a learning rate of 0.005.
We used the n-softsync algorithm with $n = \lambda$ which is the same as ASGD.
FASGD beat SASGD in all instances, but the relative outperformance increases as the client count goes up.
Since overall staleness increases as the client count goes up, this provides evidence in favor of our hypothesis
that FASGD helps more when staleness is higher.
These results also suggest that FASGD might allow improved speedup in distributed training contexts
by helping us to increase the client count.

\begin{figure}[ht!]
  \centering
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/250_client_mnist.png}
  \end{minipage}%%
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/500_client_mnist.png}
  \end{minipage} 
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/1000_client_mnist.png}
  \end{minipage}%% 
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/10000_client_mnist.png}
  \end{minipage}%% 
  \caption{MNIST Validation cost for FASGD (Blue) and SASGD (Green),
    tested with different client counts.
    In normal reading order, the client counts used are 250, 500,
    1000, 10000.
  }
  \label{fig2} 
\end{figure}

\subsection{Bandwidth Aware FASGD}

We use the same model from before, also on MNIST.
In this section, however, we are interested in how much we can reduce bandwidth
usage while still achieving good convergence performance.

We use the B-FASGD model defined earlier with the standard FASGD model as a baseline.

As above, we divide bandwidth usage into copies of the parameters from the server to a client and
copies of the gradients from a client to the server (fetches and pushes, respectively).
We find that fetch usage can be reduced significantly without seriously impacting performance,
while reducing push usage quickly causes training to diverge.


\begin{figure}[ht!]
  \centering
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/one_million_cost_graph_copies_kfetch.png}
  \end{minipage}%%
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/one_million_parameter_copies_kfetch.png}
  \end{minipage} 
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/one_million_cost_graph_copies_kpush.png}
  \end{minipage}%% 
  \begin{minipage}[b]{0.5\linewidth}
    %\centering
    \includegraphics[scale=0.35]{figures/one_million_gradient_copies_kfetch.png}
  \end{minipage}%% 
  \caption{
    Convergence and bandwidth usage (both push and fetch)
    of B-FASGD for a variety of c-values.
    The top row shows the results of modulating just kfetch.
    The bottom row shows the results of modulating just kpush.
  }
  \label{fig2} 
\end{figure}

\vspace{-2.5mm}
\section{Future Work}
\vspace{-1mm}

We are excited to explore further extensions to this work.

We would like to expand B-FASGD by synchronizing parameters on a per-tensor basis.
Since we are dynamically choosing when to synchronize using per-parameter trailing statistics,
we could choose to update parameters more frequently when their trailing statistics
inidicate a higher likelihood of staleness, and vice versa.

We are also interested in the idea of fixing a bandwidth budget and using the gradient
statistics to dynamically allocate portions of that budget to different tensors
according to likelihood of staleness.

We would also like to explore why convergence performance is so much more sensitive
to changes in the push rate than it is to changes in the fetch rate.

Another idea would be to turn n-softsync (now you need to cite) into v-softsync,
where v is a per-tensor measure of staleness modulated by trailing gradient statistics.

Finally, we would like to more formally characterize the trade-off between bandwidth usage and convergence,
and the trade-off between accuracy of modulation (expand) and computation cost.
These last two strike us as fundamental questions worth more consideration.
There may be some interplay between this and formal work in distributed systems.

TODO: anything else in my notes I should throw in?
This list is already pretty long.

\vspace{-2.5mm}
\section{Conclusion}
\vspace{-1mm}
In summary, the key technical contributions of this work are as follows:

We introduce an algorithm (FASGD) for asynchronous gradient descent that converges faster and to lower cost than
the current state of the art. We also formally characterize the convergence properties of this algorithm.

We demonstrate that the same ideas can be used to reduce bandwidth costs in a distributed context.

We provide an open source framework for evaluating distributed training algorithms, and use this
framework to demonstrate the superior performance of FASGD on standard benchmarks (MNIST and sequential MNIST).

We expect this work to be especially important in contexts where the staleness distribution is poorly behaved.
For instance, when the training cluster is large and heterogenous, we expect FASGD to outperform SASGD even more (why)?
FASGD could also be useful for distributed training of conditional networks as in \cite{Conditional}.
If only certain tensors are updated for a given input,
it may be beneficial to selectively send per-tensor updates over the network. 

{\bf Acknowledgments}: We thank the developers of Theano (\cite{Theano}).
We also thank Yoshua Bengio for helpful discussions. 

\newpage

\nocite{*}
\bibliography{fasgd}
\bibliographystyle{fasgd}

\end{document}
